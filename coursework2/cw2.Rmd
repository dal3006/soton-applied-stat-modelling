---
title: "MATH6157 – Applied Statistical Modelling"
author:
- Ronald M. Gualan Saavedra (29021812)
- MSc. Data Science
subtitle: Assessed Assignment 2
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---
```{r}
set.seed(29021812) # Set the random number seed to the University ID of the author
```

### 1. Mixed models
**The bacteria data set (available from the MASS library in R) gives the results of tests for the presence (y) or absence (n) of the bacteria H. influenzae in a group of 50 children (ID) in Australia. Each child was given one of three treatments (trt), placebo, drug, drug+, and measurements were taken at weeks 0, 2, 4, 6 and 11.**

First, a simple preprocessing of the dataset
```{r, message=FALSE, warning=FALSE}
library(MASS)

bacteria_ds <- bacteria
bacteria_ds$ynum <- as.integer(bacteria$y == 'y')

attach(bacteria_ds)
```


**a) Fit various generalised linear models (GLMs) to the data using week, trt and ID as explanatory variables, and comment on their fit, e.g. using summary statistics and/or graphical summaries. Produce a normal probability plot for the residuals from at least one of the models you fit, and comment on its usefulness for binary (ungrouped) data, giving reasons for your comments.**

Some logistic regression models (GLM) were created trying some combinations of predictors. A logistic regression is a suitable choice for the bacteria dataset because the predictand `y` is a binary variable. 
```{r}
bacteria.glm0 <- glm(ynum ~ 1, family=binomial) # intercept
# Some models with week predictor only, for creating a plot
bacteria.glm01 <- glm(ynum ~ week, family=binomial) 
bacteria.glm02 <- glm(ynum ~ poly(week,2), family=binomial) 
bacteria.glm03 <- glm(ynum ~ poly(week,3), family=binomial) 
# Other combinations of predictors
bacteria.glm1 <- glm(ynum ~ trt, family=binomial) 
bacteria.glm2 <- glm(ynum ~ trt + week, family=binomial) 
bacteria.glm3 <- glm(ynum ~ trt + poly(week,2), family=binomial) 
bacteria.glm4 <- glm(ynum ~ trt + week + ID, family=binomial) 
# A plot to check the models involving only the week predictor
newWeek <- data.frame(week = 0:11)
plot(ynum ~ week, xlab='week', ylab='y')
lines(newWeek$week, predict(bacteria.glm0,newWeek,type="response"),lwd=2,col=2)
lines(newWeek$week, predict(bacteria.glm01,newWeek,type="response"),lwd=2,col=3)
lines(newWeek$week, predict(bacteria.glm02,newWeek,type="response"),lwd=2,col=4)
lines(newWeek$week, predict(bacteria.glm03,newWeek,type="response"),lwd=2,col=5)
legend(x='left',c('Intercept', 
               'Week',
               'Week^2',
               'Week^3'),lwd=c(2,2,2,2),col=c(2,3,4,5))
```
The plot shows an expected trend in the GLM model curves. However, the trend is not as pronounced as would be desirable, to clearly define the yes and no areas.  


After some GLMs have been fit with the bacteria dataset, it is time to evaluate how well these models fit the data. The deviance, R2, and AIC are basic metrics that help to accomplish this task. 
```{r, warning=FALSE}
modelNames = c(deparse(bacteria.glm0$formula), deparse(bacteria.glm01$formula),
               deparse(bacteria.glm02$formula), deparse(bacteria.glm03$formula),
               deparse(bacteria.glm1$formula), deparse(bacteria.glm2$formula),
               deparse(bacteria.glm3$formula), deparse(bacteria.glm4$formula))
aDeviance =    c(bacteria.glm0$deviance, bacteria.glm01$deviance,
                 bacteria.glm02$deviance, bacteria.glm03$deviance,
                 bacteria.glm1$deviance, bacteria.glm2$deviance,
                 bacteria.glm3$deviance, bacteria.glm4$deviance)
listR2 =    c(cor(bacteria.glm0$fitted, ynum)^2, cor(bacteria.glm01$fitted, ynum)^2,
              cor(bacteria.glm02$fitted, ynum)^2, cor(bacteria.glm03$fitted, ynum)^2,
              cor(bacteria.glm1$fitted, ynum)^2, cor(bacteria.glm2$fitted, ynum)^2,
              cor(bacteria.glm3$fitted, ynum)^2, cor(bacteria.glm4$fitted, ynum)^2)
listAIC = c(bacteria.glm0$aic, bacteria.glm01$aic, bacteria.glm02$aic,
            bacteria.glm03$aic, bacteria.glm1$aic, bacteria.glm2$aic,
            bacteria.glm3$aic, bacteria.glm4$aic)
table_comparison_glm = data.frame(Formula=modelNames, 
                                  Deviance=round(aDeviance,2),
                                  R2=round(listR2,3),
                                  AIC=round(listAIC,2))
table_comparison_glm
```

Similarly, the performance of the fitted models can be compared by means of a Chi-squared test with the anova method.
```{r}
anova(bacteria.glm0, bacteria.glm01, bacteria.glm02, bacteria.glm03, 
      bacteria.glm1, bacteria.glm2, bacteria.glm3, 
      bacteria.glm4, test="Chisq")
```

According to the goodness-of-fit metrics the complete model (using all the predictors) obtains the lowest deviance and the highest  R2. Obviously it also has the highest AIC, as a consequence of the considerable amount of variables used by the model. The anova comparison using the Chi-squared test, indicates that the complete model is the most significant because it considerably reduces the deviance in comparison with the simpler models. This is an expected result because it uses a considerable amount of parameters (49 extra dummy variables for the ID predictor). Despite the good metrics, the complete model implements a naive fixed effect for the individuals in the dataset, and therefore it can perform predictive inference only for those individuals. This is not very useful for most of the applications.

Another relevant way to assess the goodness-of-fit of a regression model is by  looking at the residual plots. The residuals plot of the best model is included below.
```{r}
plot(bacteria.glm4, which=1); 
```

The Residuals versus Fitted plot does not show a strong pattern or outliers, which does not suggest poor fit for any particular observation or subset of observations.  

_Normal probability plot for the residuals_
```{r}
plot(bacteria.glm4,which=2)
```

The normal Q-Q plot shows that the standard deviance residuals are not normally distributed. It illustrates the presence of heavy tails. However, these residuals don't have to be normally distributed for this type of regression model, because logistic regression relaxes the assumption of normality. Hence, the normal Q-Q plot is not relevant for this type of regression. The reason for this is that, in logistic regression the dependent variable is not evenly distributed around the regression curve. That applies for linear regression but does not for binary logistic regression. In binary logistic regression the dependent variable only has two values (0 or 1). Thus, the residuals cannot be normally distributed. 



**(b) Now fit various generalised linear mixed models (GLMMs), including random effects. Assess the fit of each model you try. Qualitatively compare your models to the best GLM fit (i.e. informally compare them)**

```{r, message=FALSE, warning=FALSE}
library(lme4) # lme4 package for mixed model fitting
bacteria.glmm0 <- glmer(ynum ~ (1 | ID), family=binomial)
bacteria.glmm1 <- glmer(ynum ~ (week | ID), family=binomial)
bacteria.glmm2 <- glmer(ynum ~ trt + (week | ID), family=binomial)
bacteria.glmm3 <- glmer(ynum ~ trt + (1|ID), family=binomial)
bacteria.glmm4 <- glmer(ynum ~ trt + (1+week|ID), family=binomial)
bacteria.glmm5 <- glmer(ynum ~ trt + week + (1|ID), family=binomial)
bacteria.glmm6 <- glmer(ynum ~ week + (1|trt/ID), family=binomial)

anova(bacteria.glmm1, bacteria.glmm2, bacteria.glmm3, bacteria.glmm4, 
      bacteria.glmm5, bacteria.glmm6, test='Chisq')
```

Some observations regarding fitting GLMMs

- Some models failed to converge. This is because those models have too many factors and not a big enough sample size, and cannot be fit  [[GuideToMM](http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html)].
- According to the anova comparison, the best GLMMs are bacteria.glmm6, bacteria.glmm1, and bacteria.glmm5. The model achieving the best p-value (bacteria.glmm6) uses random effects on `trt` and `ID`. 


Some diagnostics

```{r}
modelNames = c(paste('glmm0: ', deparse(formula(bacteria.glmm0))),
               paste('glmm1: ', deparse(formula(bacteria.glmm1))),
               paste('glmm2: ', deparse(formula(bacteria.glmm2))),
               paste('glmm3: ', deparse(formula(bacteria.glmm3))),
               paste('glmm4: ', deparse(formula(bacteria.glmm4))),
               paste('glmm5: ', deparse(formula(bacteria.glmm5))),
               paste('glmm6: ', deparse(formula(bacteria.glmm6))))
vectorDeviance = c(deviance(bacteria.glmm0),
                   deviance(bacteria.glmm1),
                   deviance(bacteria.glmm2),
                   deviance(bacteria.glmm3),
                   deviance(bacteria.glmm4),
                   deviance(bacteria.glmm5),
                   deviance(bacteria.glmm6))
vectorR2 =  c(cor(fitted(bacteria.glmm0), ynum)^2,
              cor(fitted(bacteria.glmm1), ynum)^2,
              cor(fitted(bacteria.glmm2), ynum)^2,
              cor(fitted(bacteria.glmm3), ynum)^2,
              cor(fitted(bacteria.glmm4), ynum)^2,
              cor(fitted(bacteria.glmm5), ynum)^2,
              cor(fitted(bacteria.glmm6), ynum)^2)
vectorAIC =    c(AIC(bacteria.glmm0),
                 AIC(bacteria.glmm1),
                 AIC(bacteria.glmm2),
                 AIC(bacteria.glmm3),
                 AIC(bacteria.glmm4),
                 AIC(bacteria.glmm5),
                 AIC(bacteria.glmm6))
vectorBIC =    c(BIC(bacteria.glmm0),
                 BIC(bacteria.glmm1),
                 BIC(bacteria.glmm2),
                 BIC(bacteria.glmm3),
                 BIC(bacteria.glmm4),
                 BIC(bacteria.glmm5),
                 BIC(bacteria.glmm6))
table_comparison_lmm = data.frame(name=modelNames,
                                  deviance=round(vectorDeviance,2),
                                  R2=round(vectorR2,2),
                                  AIC=round(vectorAIC,2),
                                  BIC=round(vectorBIC,2))
table_comparison_lmm
```

From the above comparison table, the best models by metric are:

- Deviance: bacteria.glmm1
- R2: bacteria.glmm1
- AIC: bacteria.glmm5
- BIC: bacteria.glmm0

Based on the above metrics the best linear mixed models are: 

- bacteria.glmm1: `ynum ~ (week|ID)`
- bacteria.glmm5: `ynum ~ trt + week + (1|ID)`



An quick comparison of the GLMMs with the best GLM fit can be performed by using a Chi-squared test with the anova function.
```{r}
anova(bacteria.glmm0, bacteria.glmm1, bacteria.glmm2, bacteria.glmm3,
      bacteria.glmm4, bacteria.glmm5, bacteria.glmm6, bacteria.glm4, test='Chisquared')
```

This test highlights the same three GLMMs chosen in the previous exercise (bacteria.glmm1, bacteria.glmm5 and bacteria.glmm6) and the best GLM (bacteria.glm4). Now we can perform a qualitative comparison of these models using the residual plots.

```{r}
par(mfrow=c(1,1))

# Combined plot
plot(residuals(bacteria.glmm1)~fitted(bacteria.glmm1), pch=0, col=2,
     main='Three models', xlab='Fitted', ylab="Residuals" )
points(residuals(bacteria.glmm5)~fitted(bacteria.glmm5), pch=1, col=3)
points(residuals(bacteria.glm4)~fitted(bacteria.glm4), pch=2, col=4)
legend("topleft", c("glmm1","glmm5","glm4"), 
       col=2:4, pch=0:2)
abline(h=0, lty=2, col='gray')

# Individual residuals plot
plot(residuals(bacteria.glmm1)~fitted(bacteria.glmm1), pch=0, col=1,
     main='glmm1', xlab='Fitted', ylab="Residuals" )
abline(h=0, lty=2, col='gray')
plot(residuals(bacteria.glmm5)~fitted(bacteria.glmm5), pch=1, col=1,
     main='glmm5', xlab='Fitted', ylab="Residuals" )
abline(h=0, lty=2, col='gray')
plot(residuals(bacteria.glm4)~fitted(bacteria.glm4), pch=2, col=1,
     main='glm4', xlab='Fitted', ylab="Residuals" )
abline(h=0, lty=2, col='gray')
```

From the plots above, it can be drawn that the three models have similar residuals, with only subtle differences among them. Since we are plotting residuals, the comparison would be easy if most of the points of one of the models would be near to the 0 line. This is barely happening with a couple of points linked to the glm4, that as we saw in the previous exercise has the best fit because uses the ID as a predictor.   


**(c) Perform an appropriate hypothesis test to compare models with and without a random intercept (i.e. models including trt, week and with/without a random effect for ID).**

An approach similar to the hypothesis test introduced in the Workshop 8 is used.

```{r, warning=FALSE}
# Function to calculate the p-value estimation based on a bootstraping test
# for comparing a simple linear model with a predictor and a LMM adding 
# random effects to that predictor
# Adapted from worksheet8
boostrap_test <- function(dependantVar,predictorVar,clusterVar,dataset,N){
  likelihood_ratio <- NULL
  formula_null <- paste(dependantVar,'~', predictorVar)
  formula_null_2 <- paste('y2~', predictorVar)
  formula_alt <-  paste(dependantVar, '~', predictorVar,'+(', predictorVar, '|', clusterVar, ')')
  formula_alt_2 <- paste('y2~', predictorVar,'+(', predictorVar, '|', clusterVar, ')')
  
  tmpDataset <- dataset
  
  for(k in 1:N) {
    tmpDataset$y2 <- simulate(lm(formula=formula_null, data=dataset))[[1]] # simulate data from the null fitted model
    boot_null <- lm(formula=formula_null_2, data=tmpDataset) # refit the null model to the "new data"
    boot_alt <- lmer(formula=formula_alt_2, data=tmpDataset) # fit the alternative model to the "new data"
    likelihood_ratio[k] <- as.numeric(2* (logLik(boot_alt) - logLik(boot_null))) # calculate the likelihood ratio in favour of the alternative
  }
  model_null <- lm(formula=formula_null, data=dataset)
  model_alt <- lmer(formula=formula_alt, data = dataset, REML = T)
  result <- mean(likelihood_ratio > as.numeric(2* (logLik(model_alt) - logLik(model_null)))) 
  # p-value of 0 - are observed result is extreme under the null hypothesis of no random effect, and hence we can reject the null (i.e. there is evidence to include a random effect)
  return(result)
}

testA<-boostrap_test('ynum', 'trt', 'ID', bacteria_ds, 100)
testB<-boostrap_test('ynum', 'week', 'ID', bacteria_ds, 100)
table_boostrap_test <- data.frame(randomEffect=c('trt','week'),
                                  pValue=c(testA,testB))
table_boostrap_test
```
- By debugging the bootstrap test function, it was noticed that the bootstrap alternative models were performing worst than the null models, i.e. likelihood_ratio with negative values.
- The bootstraping tests resulting in a p-value of 0.00, which suggests that the null hypothesis can be rejected. Hence, there is some evidence to include a random effect for both variables. 


**(d) One advantage of the random intercept model is that you can predict for unseen subjects (new levels of ID). Using this model, predict the response for all five weeks for three new subjects, one who has been given the placebo, one who has been given the drug, and one who has been given drug+.**

```{r}
placeboNewDs2 <- data.frame(ID = c(rep('A01',5),rep('A02',5),rep('A03',5)),
                            week = rep(c(0,2,4,6,11),3),
                            trt = c(rep('placebo',5),rep('drug',5),rep('drug+',5)))
placeboNewDs2
lmm_model <- lmer(ynum ~ trt + week + (1|ID), REML = F)
plaPred <- predict(lmm_model,placeboNewDs2,allow.new.levels=TRUE)
plot(plaPred, ylab="prediction")

```

The plot shows a reasonable result: the predictions for the subjects taking drugs are closer to 0 than the subject who is taking the placebo. However, nothing can be said about the effect of the drugs in the new subjects, because the predictions are located in the area between 0.58 and 1. Thus, the mixed model predicts that the new subjects will have the bacteria during the 5 weeks.  

**(e) If we ignore the variable week, we can create a binomial response for each child. Aggregate the data across weeks to create a new binomial response for each child (level of ID) and fit a GLMM with random intercept to this new data set. Do the two treatments differ from placebo for this grouped data?**

First aggregate the data and calculate the new response. The new response is represented by the amount of occasions in which a child has or has not the bacteria.  
```{r}
# Aggregate data
countY <- table(ID,ynum) # tabulate data by ID and response
yy <- countY[,2] # count yes
yn <- countY[,1] # count no
ybind <- cbind(yy,yn) 
# Convert to dataframe
bacteria_agg <- unique(data.frame(ID,trt)) # uniques candidates and treatment by candidate 
bacteria_agg$ybind <- ybind
bacteria_agg$drugs <- as.numeric(bacteria_agg$trt!='placebo') # combine the drugs in one
# aggregated dataframe
head(bacteria_agg)
```


Fit a GLMM with random intercept
```{r}
bacteria_agg.glmm <- glmer(ybind ~ drugs + (1|ID), family=binomial, data=bacteria_agg)
summary(bacteria_agg.glmm)
```

The fixed effect of the parameter `drugs` (which combines `drug` and `drug+`) is not big enough to imply a big change in the response variable. This is reaffirmed by the insignificant `p-value=0.0593`. Thus, neither drug treatment has a significant effect in lowering the presence of the bacteria H. influenza compared with the placebos.

```{r}
detach(bacteria_ds)
```


### 2. Smooth regression
**The aatemp data set (available from the faraway library in R) gives the annual mean temperature (in degrees Fahrenheit) in Ann Arbor, Michigan, for 115 years between 1854 and 2000. We will fit various smooth models to the subset of the data which excludes the first three observations (from years 1854, 1855 and 1871). From 1881 onwards, we have mean temperatures for every year.**

**(a) Start by creating two new variables that contain the year and temperature data that we wish to model. Plot the data.**

**(b) Fit a simple linear model with temp∼year. Add the fitted regression line to your plot. Is there evidence for a non-zero slope parameter? What is a main restriction of this modelling approach?**

```{r, message=FALSE, warning=FALSE}
library(faraway)
aatemp_ds <- aatemp[aatemp$year>=1881,] # Filter time period
attach(aatemp_ds)
# Create a plot with a simple linear trend 
plot(year,temp) # plot
aatemp.lm <- lm(temp~year) # linear model
lines(year,fitted(aatemp.lm), col=2, lwd=2) # Fitted regression line
summary(aatemp.lm)$coefficients # For checking the evidence of a non-zero slope 
```

- Based on the p-value (0.000351) it can be drawn that there is a strong evidence of a non-zero slope parameter.
- The main restriction of this model approach is that is unable to capture non-linear patterns.


**(c) A smoothing method not discussed in lectures is “local weighted regression” (often called Loess or Lowess). For prediction at a point x, this smoother fits polynomial regression models using data “near” x, weighted by the distance from x (similar in principle to the weighting in a kernel smoother). It can be thought of as a generalisation of a “moving average” smoother, with simple averaging of data local to x replaced by a local least squares fit. It is implemented in the package loess in R. The proportion of data points used for each local fit is set by the parameter span (> 0).**

**i. Read the help file on the loess function and then apply the function to the data using the default span = 0.75. Add the fitted smoother to your plot.**

```{r}
aatemp.loess <- loess(temp~year,span=0.75) # loess smoother
# Create plot
plot(year,temp)
lines(year,fitted(aatemp.lm), col=2, lwd=2)
lines(year,fitted(aatemp.loess), col=3, lwd=2)
legend("topleft", c("linear","loess"), col=2:3, lwd=rep(2,2))
```



**ii. The bootstrap package in R contains function crossval, which can be used for cross-validation with a reasonably general class of models (see also Worksheet 5). Read the help file for this function, and use it to perform replicated cross-validation to choose the value of span. Add the best fitted Loess smoother to your plot. Comment on the differences between the three smoothers you have fitted.**

```{r, warning=FALSE}
library(bootstrap)

# function for fitting the loess smoother (to be cross-validated)
loess.fit <- function(x,y,span){
  loess(y~x,span=span)
}

# function for producing the predictions for the loess smoother
loess.predict <- function(fit, x){
  predict(fit, x)
}

param.grid <- seq(.1, .9, by =.01) # range of span parameter to test
mse <- matrix(NA, nrow = length(param.grid), ncol = 2) #matrix to store the cv-mse for each parameter
R <- 10 # number of repetitions
K <- 10 # K-fold

for (b in 1:length(param.grid)){ # loop through all bandwidths
  cv.R <- rep(0, R) # to store the mse for each repetition
  for(r in 1:R) { # repeat K-fold CV R times
    loess.cv <- crossval(year,temp,loess.fit,loess.predict, 
                         span=param.grid[b], ngroup=K)  
    cv.R[r] <- mean((loess.cv$cv.fit - temp)^2, na.rm = T)
  }
  mse[b, ] <- c(param.grid[b], mean(cv.R))   
}

# find the span with minimum MSE
bestSpan <- mse[which.min(mse[,2]), 1] 
sprintf('Best spam parameter: %.2f', bestSpan)
aatemp.bestLoess <- loess(temp~year,span=bestSpan)

# Create plot
plot(year,temp)
lines(year,fitted(aatemp.lm), col=2, lwd=2)
lines(year,fitted(aatemp.loess), col=3, lwd=2)
lines(year, predict(aatemp.bestLoess,year), col=4, lwd=2)
legend("topleft", c("linear","loess","best-loess"), col=2:4, lwd=rep(2,3))
```

The figure above shows three smoothers with different complexity. The linear model is the simplest smoother. The optimal span parameter leads to a complex loess model (best-loess). The loess model with the default spam parameter has an intermediate complexity compared to the other two models. In terms of interpretation: The linear model identifies the increasing temperature trend of the last 120 years. The intermediate loess in addition to identify the increasing trend, also detects an unusual warm period between 1930 and 1970. Finally, the best loess model  seems to be correctly capturing the phenomena captured by the simpler models with additional detail. It also captures the complex (usually seasonal) pattern of the temperature variable. Thus, the difference in complexity of the studied models is translated to a difference in the levels of the patterns been captured. 



**(d) Also fit a kernel smoother to this data, using cross-validation to choose the smoothing parameter. You can use the cv function from lectures. Add the best smoother to your plot.**

```{r, warning=FALSE}
library(cvTools) # this package has some useful tools for cross-validation
## function to perform repeated K-fold cross-validation using predictions supplied by the function smooth.function
## dataset - data
## smooth.function - smoother that returns test set predictions
## bandwidth.grid - bandwidths to assess
## K - number of folds for cross-validation
## R - number of repetitions
cv <- function(dataset, smooth.function, bandwidth.grid, K = 10, R = 10)
{
  n <- nrow(dataset)
  mse <- matrix(NA, nrow = length(bandwidth.grid), ncol = 2)
  for (b in 1:length(bandwidth.grid)){ # loop through all bandwidths
    cv.R <- rep(0, R)
    for(r in 1:R) { # repeat K-fold CV R times
      folds <- cvFolds(n, K) # define the K-folds (this is where we are using cvTools)
      cv.value <- rep(0, K)
      for(k in 1:K) { # loop through the K-folds, using each as a test set in turn
        train <- dataset[folds$subsets[folds$which != k], ]
        test <- dataset[folds$subsets[folds$which == k], ]
        funcargs <- list(reg.x = train[, 1], reg.y = train[, 2], x.p = test[, 1], 
                         h = bandwidth.grid[b]) # args to pass to our smoother
        temp.pred <- do.call(smooth.function, funcargs) # train on K-1 folds, predict 1 fold
        cv.value[k] <- mean((test[order(test[, 1]), 2] - temp.pred)^2, na.rm = T)
      }
      cv.R[r] <- mean(cv.value) 
    }
    mse[b, ] <- c(bandwidth.grid[b], mean(cv.R))   
  }
  colnames(mse) <- c("Bandwidth", "MSE")
  return(mse)
}
## functions to fit a kernel smoother (to use with our cv)
ksmoothCV <- function(reg.x, reg.y, x.p, h) {
  return(ksmooth(x = reg.x, y = reg.y, x.point = x.p, kernel = "normal", bandwidth = h)$y)
}

## Test CV error for the smoother 
aatemp.smoother.cv <- cv(aatemp_ds, ksmoothCV, seq(.7, 8, by =.1), R = 100) # TODO
#save(aatemp.smoother.cv, file="data/aatemp.smoother.cv.RData")
#load("data/aatemp.smoother.cv.RData")
## find the bandwidth with minimum MSE
mb <- aatemp.smoother.cv[which.min(aatemp.smoother.cv[,2]), 1] 
## Fit best model
aatemp.bestKsmooth <- ksmooth(x = year, y = temp, kernel = "normal", 
                              bandwidth = mb, x.points = year)

# Plot points and models
plot(year,temp)
lines(year,fitted(aatemp.lm), col=2, lwd=2)
lines(year,fitted(aatemp.loess), col=3, lwd=2)
lines(year, fitted(aatemp.bestLoess,year), col=4, lwd=2)
lines(year, aatemp.bestKsmooth$y, col=5, lwd=2)
legend("topleft", c("linear","loess","best-loess","best-ksmooth"), col=2:5, lwd=rep(2,4))

```


**(e) For each smoother (the linear model, the two Loess smoothers and the kernel smoother), work out the R2 value. By comparing the R2 values and the plots, comment on the differences between the various smoothers (e.g., which appears to have the smallest/largest equivalent degrees of freedom?).**



```{r}
modelNames <- c('linear','loess','best-loess', 'best-ksmooth')
smoothers_R2 <- c(cor(temp, fitted(aatemp.lm) )^2,
                 cor(temp, fitted(aatemp.loess) )^2,
                 cor(temp, fitted(aatemp.bestLoess) )^2,
                 cor(temp, aatemp.bestKsmooth$y )^2)
#df.smother.R2 <- data.frame(model=modelNames, R2=smoothers_R2)
cbind(model=modelNames, R2=smoothers_R2)
```



Some observations:

- The considered smoothers present different levels of complexity, which leads to think in terms of fitting and smoothing (over-fitting versus over-smoothing).
- The model with the highest R2 is the best-loess (`R2=0.4613`)
- The degrees of freedom of a model is related to its complexity. A model with a lot of parameters usually have a small number of degrees of freedom.
- Based on the plots and the summary of the models, the degrees of freedom of each model are approximately as follows:
    - lm: 110 (over-smooth)
    - loess: Equivalent Number of Parameters: 4.37. Therefore, DF~107
    - best-loess: 14.99. Therefore, DF~97
    - best-ksmooth: Not available. It might have less DF than loess-2 (over-fit)
- As expected simpler models, such as the linear model, retain more degrees of freedom
- The linear model has the biggest number of degrees of freedom in this exercise
- It is estimated that the ksmooth has the shortest number of degrees of freedom



### 3. Design and smoothing non-normal data
**In this question, you will generate an experimental design, collect some (simulated) data and fit generalised additive models. To ensure I get the same results as you when I run your code, you must set the random number seed using your student ID, e.g. set.seed(12345678), at the start of your answer for this question. After you are satisfied with your answer, I suggest you run all the code again, to ensure it is reproducible.**

**(a) Generate a maximin Latin hypercube design with n = 100 in 10 variables. Plot the one-dimensional projections of your design, and comment on their distributions.**

```{r}
library(SLHD) # this package will generate LHDs
# A 10D Latin hypercube design with 100 runs

Mmdesign <- maximinSLHD(1, 100, 10) #TODO
#save(Mmdesign, file="data/Mmdesign.RData")
#load("data/Mmdesign.RData")

par(mfrow=c(1,2), pty = "s")
boxplot(Mmdesign$StandDesign, names = paste(rep('x',10),1:10, sep=''), 
        main='1D projections', xlab='Variable', ylab='Standard design')
hist(Mmdesign$StandDesign, main='Histogram', xlab='Standard design')
```

The bar chart plot representing the 1D projections of the Maximin Latin Hypercube design, shows that the values for each of the 10 variables are uniformly distributed. This is verified with the histogram.



**(b) Generate 1000 random Latin hypercube designs and 1000 designs with points randomly sampled from independent uniform distributions for each variable. For each design, calculate the minimum inter-point Euclidean distance. For each set of designs, plot the distribution of these distances and then graphically compare them to the minimum inter-point distance of your maximin design.**

The chunk below contains the code regarding the creation of:

- 1000 random Latin hypercube designs (LDH-random), and
- 1000 designs with points randomly sampled from independent uniform distributions for each variable (LDH-uniform)

and the calculation and plotting of the minimum inter-point Euclidean distances for each set of LHD.

```{r, warning=FALSE}
# 1000 Random LHD
lhd_random <- matrix(runif(1000), nrow = 100, ncol = 10)

# 1000 LHD with points randomly sampled from independent uniform distributions for each variable
lhs_uniform <- matrix(data = NA, nrow = 100, ncol = 10)
for(col in 1:dim(lhs_uniform)[2]){
  lhs_uniform[,col] <- sample(1:100)
}
lhd_uniform <- apply(lhs_uniform, 2, 
                    function(x) runif(length(x), min = ((x - 1) / 100), max = x / 100))

# Function that calculates a vector with the shortest closest Euclidean distance for each row, using brute force
# param aMatrix: the LHD matrix
calculateMinimumInterpointEuclideanDistance <- function(aMatrix){
  NROWS = dim(aMatrix)[1]
  NCOLS = dim(aMatrix)[2]
  minDistance <- rep(NA, NROWS)
  # Look for the closest point to each of the rows in the matrix
  for(i in 1:dim(aMatrix)[1]){
    actual = aMatrix[i,] # actual point 
    distances = rep(NA,dim(aMatrix)[1]-1) # array of distances with the other points
    for(j in 1:dim(aMatrix[-i,])[1]){
      other = aMatrix[-i,][j,]
      distances[j] = dist(rbind(actual,other)) # Euclidean distance between two points
    }
    minDistance[i] = min(distances)
  }
  
  return(minDistance)
}
# Distances
d1 = calculateMinimumInterpointEuclideanDistance(lhd_random)
d2 = calculateMinimumInterpointEuclideanDistance(lhd_uniform)
d3 = calculateMinimumInterpointEuclideanDistance(Mmdesign$StandDesign)

# Plot the Smoothed density estimates of these distances
library(ggplot2)
df1 <- data.frame(distance = d1, name='LHD-random')
df2 <- data.frame(distance = d2, name='LHD-uniform')
df3 <- data.frame(distance = d3, name='LHD-minimax')
#Combine the dataframes into one.
dfDistances <- rbind(df1, df2, df3)
#Make the density plots
ggplot(dfDistances, aes(distance, fill = name)) + geom_density(alpha = 0.2)

```

- The distance distribution for the random LHD and the uniform LHD are similar.
- Clearly, the inter-point distances of the LHD-minimax are bigger than the other Latin hypercubes.



**The R workspace math6157Cwk2.RData can be downloaded from Blackboard. It can be loaded into R using the command load("math6157Cwk2.RData"). The workspace contains one function, math6157Cwk2Data(), which takes one argument, a dataframe with 10 columns. It returns a vector of binomial responses, in the form of the number of successes from 10 trials.**

**(c) Use the function math6157Cwk2Data() to generate data from your maximin design. Note that the function returns random data, so different responses will be generated each time you run the function. Therefore, only run this function once to get your data.**

```{r}
load("math6157Cwk2.RData")
#load('data/dfMDM.RData')

dfMDM <- data.frame(Mmdesign$StandDesign) #maximin design matrix
set.seed(29021812) # Set the random number seed to the University ID of the author
dfMDM$y <- math6157Cwk2Data(dfMDM) # execute only once
colNames <- c()
for(i in 1:10) colNames <- c(colNames, paste("x", i, sep = ""))
colNames <- c(colNames,'y')
names(dfMDM) <- colNames
#save(dfMDM, file='data/dfMDM.RData') #For avoinding changes in the data
```



**(d) For your data, plot each of the 10 variables against the response. Comment on any trends.**

```{r}
par(mfrow=c(1,2), pty='s')
for(i in 1:10){
  plot(dfMDM[,i], dfMDM$y, xlab=names(dfMDM)[i], ylab='y')
}
```
It is hard to identify some pattern as the points are scattered and most of them are located in the upper bound. However, there seems to be be some trend in the plots:

- response ~ x2
- response ~ x9



**(e) Read the help file for gam and then use the function to fit a GAM that includes all 10 variables in the training data. Produce effect plots for this model, including partial residuals. From the summary of the fitted model object and the plots, comment on which terms seem most important.**

```{r, warning=FALSE}
library(mgcv)
smooth.formula <- as.formula(paste("y ~ ", paste("s(x", 1:10, ")", 
                                                      sep="", collapse = "+")))
## short-cut to generate the model formulae
exercise.gam <- gam(smooth.formula, data = dfMDM) 
## fit additive model, smoother in each dimension
summary(exercise.gam)
```
Based on the p-values of the smooth terms for the gam model, there seems to be 3 relevant predictors:

- s(x2),
- s(x3), 
- s(x9) 

Effect plots and partial residuals
```{r}
par(mfrow = c(1, 2), pty = "s") # 2 plots per row
for(i in 1:10) plot(exercise.gam, residuals=T, rug=F, select=i)
```

From the plots above, it can be seen that the terms that seem to be important, are: x2, x3, x6, x7, x9.


**(f) Use the anova function (and test = "Chisq") to find a reduced model that provides an adequate fit for the data. Produce effect plots for this model.**
```{r}
## fit a simpler models and perform model comparison
exercise.gam2 <- gam(y~s(x2)+s(x3)+s(x6)+s(x7)+s(x9), data=dfMDM)
anova(exercise.gam2, exercise.gam, test = "Chisq") # intermediate model vs original model
exercise.gam3 <- gam(y~s(x2)+s(x3)+s(x9), data = dfMDM)
anova(exercise.gam3, exercise.gam2, test = "Chisq") # simpler model vs intermediate model
```

According to the first comparison (intermediate model vs original model), there is not  evidence (p-test=0.7042) that the original model produces a significant improvement over the intermediate model. Thus the intermediate model (Null model) is kept.  In a second anova analysis, again there is no evidence to discard the null model. Thus, we achieved a simplified version with only 3 variables (x2, x3, x9).

Produce effect plots for the simple model.
```{r}
par(mfrow = c(1, 2), pty = "s") # 2 squared plots per row
for(i in 1:3) plot(exercise.gam3, residuals=T, rug=F, select=i)

```







